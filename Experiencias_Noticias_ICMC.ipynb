{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mNDkXZy_TTE4",
        "qm6BbtU5oPrV",
        "tf-Hf9SamVGU",
        "GLeF-b7InuiX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependências\n"
      ],
      "metadata": {
        "id": "JBALOe0fRjoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlGnuAJYRO5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ade29e7-93b1-4eec-ef48-a5cbc8a7e539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/2.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain \"openai<1.0.0\" tiktoken \"pinecone-client[grpc]\" apache_beam mwparserfromhell cohere python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregar *API keys*"
      ],
      "metadata": {
        "id": "1UujmB7BRos7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from dotenv import load_dotenv\n",
        "files_uploaded = files.upload()\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "aicpbQ76RrU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "86e096c9-c338-4e00-e80b-475cc69ca941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fddafeb5-07ef-439c-a90d-260231f26a97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fddafeb5-07ef-439c-a90d-260231f26a97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env to .env\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "UsSogVxqU4vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerais\n",
        "import os\n",
        "import datetime\n",
        "from uuid import uuid4\n",
        "import tiktoken\n",
        "import pinecone\n",
        "\n",
        "# LangChain\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema import BaseOutputParser"
      ],
      "metadata": {
        "id": "hIgyRluGU8sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32877e7b-4caa-4606-e943-3a85a2ac8025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição de funções"
      ],
      "metadata": {
        "id": "6te7Gzgtp3Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de comprimento (len) baseado em um tokenizer\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "e1DJWs2CVwtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de inserção de dados em um index do Pinecone\n",
        "def insert_index(index, texts, metadatas, embeddings_model):\n",
        "    # Obtem ids unicos\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    # Realiza embedding dos textos\n",
        "    embeds = embeddings_model.embed_documents(texts)\n",
        "    # Insere no índice\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))"
      ],
      "metadata": {
        "id": "tzbBTYBtWJJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que itera sobre os chunks para inseri-los no Pinecone\n",
        "def insert_pinecone(index, chunks, embedding_model):\n",
        "  batch_limit = 100\n",
        "  texts = []\n",
        "  metadatas = []\n",
        "\n",
        "  # Chunks obtidos na módulo de divisao\n",
        "  for chunk in chunks:\n",
        "      # Obter texto e metadados do chunk\n",
        "      chunk_text = chunk.page_content\n",
        "      chunk_metadatas = chunk.metadata\n",
        "      chunk_metadatas[\"year\"] = datetime.datetime.now().year\n",
        "      chunk_metadatas[\"text\"] = chunk_text\n",
        "\n",
        "      # Adiciona a lista de textos e metadados\n",
        "      texts.append(chunk_text)\n",
        "      metadatas.append(chunk_metadatas)\n",
        "\n",
        "      # Se ultrapassou o tamanho limite do lote (batch),\n",
        "      # insere no índice\n",
        "      if len(texts) >= batch_limit:\n",
        "          insert_index(index, texts, metadatas, embedding_model)\n",
        "          # Esvazia listas\n",
        "          texts = []\n",
        "          metadatas = []\n",
        "\n",
        "  # Se ainda restam dados a serem inseridos\n",
        "  if len(texts) > 0:\n",
        "      insert_index(index, texts, metadatas, embedding_model)"
      ],
      "metadata": {
        "id": "9lRAV_xxXWnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sources(vectorstore, query, k=3):\n",
        "    return vectorstore.similarity_search(query, k=k)"
      ],
      "metadata": {
        "id": "GJFq0ZkHPNru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alimentar banco de dados\n"
      ],
      "metadata": {
        "id": "mNDkXZy_TTE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar documentos\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "    \"https://jornal.usp.br/ciencias/ciencias-isolamento-e-coesao-dos-grupos-de-direita-facilitaram-propagacao-coordenada-nas-eleicoes/\",\n",
        "    \"http://www.saocarlos.usp.br/atencao-a-saude-mental-e-inclusao-na-universidade/\",\n",
        "    \"https://cemeai.icmc.usp.br/projeto-tematico-une-ciencia-de-dados-e-sociologia-no-mapeamento-da-criminalidade/\",\n",
        "    \"https://cemeai.icmc.usp.br/o-avanco-das-pesquisas-matematicas-com-foco-no-espectro-autista/\"\n",
        "])\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "# Definição do tokenizer\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "# Definição do text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Divide o documento em chunks\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Verifica se é preciso criar o índice\n",
        "index_name = \"noticias-icmc\"\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # Cria novo índice\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric=\"cosine\", # Metrica de busca\n",
        "        dimension=1536  # Dimensão do embedding. 1536 para text-embedding-ada-002\n",
        "    )\n",
        "\n",
        "# Carrega o índice\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Insere chunks no Pinecone\n",
        "insert_pinecone(index, chunks, embedding_model)"
      ],
      "metadata": {
        "id": "DEccXXxcTS1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construção de *chains*"
      ],
      "metadata": {
        "id": "-ltyInxQBrIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain1: k = 3"
      ],
      "metadata": {
        "id": "qm6BbtU5oPrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = \"noticias-icmc\"\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"Use os seguintes trechos de contexto para responder à pergunta no final.\n",
        "Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "Contexto: {contexto}\n",
        "Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain1 = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "d5-Tsf5MoNht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain 2: k = 7"
      ],
      "metadata": {
        "id": "tf-Hf9SamVGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = \"noticias-icmc\"\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"Use os seguintes trechos de contexto para responder à pergunta no final.\n",
        "Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "Contexto: {contexto}\n",
        "Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain2 = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "JWOofXkE98qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain 3: k = 4"
      ],
      "metadata": {
        "id": "GLeF-b7InuiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = \"noticias-icmc\"\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"Use os seguintes trechos de contexto para responder à pergunta no final.\n",
        "Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "Contexto: {contexto}\n",
        "Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain3 = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "KmUamQe-nuig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiências"
      ],
      "metadata": {
        "id": "U5Q-7b9xDLDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notícia 1\n",
        "Isolamento e coesão facilitaram propagação de informações dos grupos de direita nas redes"
      ],
      "metadata": {
        "id": "Uz0JsQyPpq7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Qual a principal característica de um grupo político polarizado?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97e2438e-0bbd-4205-ca6b-d5773bad9400",
        "id": "Ua-ebJZpZk1A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A principal característica de um grupo político polarizado é o isolamento de outros grupos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"O que fez com que os grupos de esquerda tivessem menos sucesso do que os grupos de direitas nas redes sociais durantes as eleições brasileiras de 2022? A resposta deve ser curta. No final da resposta referencie a fonte das informações e seu link.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f54c2a6e-53e2-4eba-a925-8f4e5f93315f",
        "id": "ajiumzIbrTfV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Os grupos de esquerda tiveram menos sucesso nas redes sociais durante as eleições brasileiras de 2022 devido à sua estrutura hierárquica menos clara e à sua integração com outras comunidades online. Por outro lado, os grupos de direita eram mais isolados e coesos internamente, o que permitiu uma propagação coordenada de informações mais rápida e eficiente. Isso é evidenciado pela análise matemática das redes feita pelos pesquisadores do Instituto de Ciências Matemáticas e de Computação (ICMC) da USP. Fonte: Jornal da USP - https://jornal.usp.br/ciencias/ciencias-isolamento-e-coesao-dos-grupos-de-direita-facilitaram-propagacao-coordenada-nas-eleicoes/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"O que fez com que os grupos de esquerda tivessem menos sucesso do que os grupos de direitas nas redes sociais durantes as eleições brasileiras de 2022? No final da resposta cite a fonte das informações.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "c0eea8ca-05da-4156-dcc1-3a7262700fa7",
        "id": "IBBiPG0fPOhy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'De acordo com a pesquisa realizada no Instituto de Ciências Matemáticas e de Computação (ICMC) da USP, os grupos de esquerda apresentaram menos sucesso nas redes sociais durante as eleições brasileiras de 2022 devido a dois fatores principais. Primeiro, os grupos de esquerda eram maiores em tamanho e tinham uma estrutura hierárquica menos clara, o que os tornava mais descentralizados. Em contraste, os grupos de direita eram mais isolados e coesos internamente, com uma hierarquia mais rígida e controlados por um número menor de influenciadores. Essa estrutura hierárquica mais clara nos grupos de direita permitia uma propagação coordenada de informações mais rápida e eficiente. Além disso, os grupos de esquerda estavam mais misturados com outras comunidades on-line, enquanto os grupos de direita eram mais isolados. Essas conclusões foram detalhadas em um artigo na revista científica Journal of Physics: Complexity, publicado em 13 de setembro de 2023. A fonte das informações é o Jornal da USP. \\n\\nFonte: https://jornal.usp.br/ciencias/ciencias-isolamento-e-coesao-dos-grupos-de-direita-facilitaram-propagacao-coordenada-nas-eleicoes/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notícia 2\n",
        "\n",
        "Atenção à Saúde Mental e Inclusão na Universidade"
      ],
      "metadata": {
        "id": "Yx8UpR_ypwxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wxMFYxcec_SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Ocorreu algum evento entre os dias 3 e 4 de outubro de 2023?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "234cc539-9541-474b-db10-e8529d05b779",
        "id": "p8Fo4bsyo7Uw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Não é possível determinar se ocorreu algum evento entre os dias 3 e 4 de outubro de 2023 com base nos trechos de contexto fornecidos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain2.invoke({\"pergunta\": \"Ocorreu algum evento entre os dias 3 e 4 de outubro de 2023?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b902a22-7fa8-4c43-8bac-74e0affde153",
        "id": "MxvsD8nblmD9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, ocorreu um evento intitulado \"Atenção à Saúde Mental e Inclusão na Universidade\" nos dias 3 e 4 de outubro de 2023.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain3.invoke({\"pergunta\": \"Ocorreu algum evento entre os dias 3 e 4 de outubro?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cK8MlM0vYdgU",
        "outputId": "47103aea-974f-47c6-f5cb-09a111dbc501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, ocorreu um evento intitulado \"Atenção à Saúde Mental e Inclusão na Universidade\" nos dias 3 e 4 de outubro.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Quando ocorreu o evento 'Atenção à Saúde Mental e Inclusão na Universidade'?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "28b7BqjWcUvC",
        "outputId": "1a7c050a-9513-4d2c-d2ad-506843da09dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"O evento 'Atenção à Saúde Mental e Inclusão na Universidade' ocorreu nos dias 3 e 4 de outubro de 2023.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sources(vectorstore, \"Ocorreu algum evento entre os dias 3 e 4 de outubro?\", 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZgPRKJwPq17",
        "outputId": "56bbc5e5-b671-497f-d73e-14101f7910c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Em entrevista ao\\xa0Jornal da USP, o pesquisador Ruben Interian, do ICMC, autor do trabalho, acredita que as conclusões do estudo podem ser aplicadas para entender os acontecimentos em Brasília em 8 de janeiro deste ano. O julgamento dos acusados de depredar as sedes dos Três Poderes, como parte de uma tentativa de golpe de Estado, começou no último dia 13 de setembro, no Supremo Tribunal Federal (STF). “É necessário esclarecer que, em um estudo dessa natureza, precisamos ter muito cuidado em apontar uma relação causal entre determinadas características da rede de interação em ambiente on-line e ações dessas pessoas no mundo real”, afirma. “Porém, parece claro que uma estrutura hierárquica e centralizada de comunicação facilita, ao menos em parte, ações radicalizadas de grandes grupos de pessoas com alto grau de coesão interna.” \\n\\n\\n\\n\\n\\n\\n\\n\\n Ruben Interian - Foto: Currículo Lattes', metadata={'language': 'pt-BR', 'source': 'https://jornal.usp.br/ciencias/ciencias-isolamento-e-coesao-dos-grupos-de-direita-facilitaram-propagacao-coordenada-nas-eleicoes/', 'title': 'Isolamento e coesão facilitaram propagação de informações dos grupos de direita nas redes – Jornal da USP', 'year': 2023.0}),\n",
              " Document(page_content='[+] Outros eventos   \\xa0 \\xa0 \\xa0\\r\\n [+] Defesas de teses  Carta de Servi√ßos da USP\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUniversidade de S√£o Paulo - Campus de S√£o Carlos \\n\\n\\n\\n\\n√Årea 1 - Av. Trabalhador s√£o-carlense, 400\\n√Årea 2 - Av. Jo√£o Dagnone, 1100, S√£o Carlos/SP\\n\\n\\n\\n\\nCréditos', metadata={'language': 'pt-BR', 'source': 'http://www.saocarlos.usp.br/atencao-a-saude-mental-e-inclusao-na-universidade/', 'title': 'Aten√ß√£o √† Sa√∫de Mental e Inclus√£o na Universidade – Portal USP S√£o Carlos', 'year': 2023.0}),\n",
              " Document(page_content='Rede que representa as interações nas redes sociais durante o dia 2 de outubro de 2022, data do primeiro turno das eleições presidenciais; as cores vermelho, azul e amarelo indicam participação na comunidade de esquerda (maior, porém descentralizada), na comunidade de direita (isolada, mas com maior coesão) e no grupo de seguidores da mídia, respectivamente - Imagem: extraída do artigo\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\nPropagação coordenada', metadata={'language': 'pt-BR', 'source': 'https://jornal.usp.br/ciencias/ciencias-isolamento-e-coesao-dos-grupos-de-direita-facilitaram-propagacao-coordenada-nas-eleicoes/', 'title': 'Isolamento e coesão facilitaram propagação de informações dos grupos de direita nas redes – Jornal da USP', 'year': 2023.0}),\n",
              " Document(page_content='As Comiss√µes de Inclus√£o e Pertencimento (CIPs) das unidades da USP de S√£o Carlos em conjunto com o Apoia USP (servi√ßo de apoio psicossocial do campus) convidam toda comunidade para participarem do evento Aten√ß√£o √† Sa√∫de Mental e Inclus√£o na Universidade, que ocorrer√° nos dias 3 e 4 de outubro de 2023, no Audit√≥rio Prof. S√©rgio Mascarenhas, no Instituto de F√≠sica da USP de S√£o Carlos (IFSC).\\nO objetivo deste evento √© promover um espa√ßo de reflex√£o sobre o modelo de cuidado em sa√∫de mental na USP S√£o Carlos, bem como discutir sobre alguns desafios atuais, relacionados ao tema do Autismo e Universidade e sobre as possibilidades de cuidado em situa√ß√µes de crise.\\nO evento conta com o apoio do Grupo Coordenador de Cultura e Extens√£o da USP S√£o Carlos\\nP√∫blico Alvo: Tod@s da comunidade USP (estudantes e servidores), comunidade externa e demais pessoas interessadas.\\nLink para inscri√ß√£o: bit.ly/eventoapoia\\nHaver√° emiss√£o de certificados.\\nPara d√∫vidas e/ou mais informa√ß√µes:¬†apoia.sc@usp.br', metadata={'language': 'pt-BR', 'source': 'http://www.saocarlos.usp.br/atencao-a-saude-mental-e-inclusao-na-universidade/', 'title': 'Aten√ß√£o √† Sa√∫de Mental e Inclus√£o na Universidade – Portal USP S√£o Carlos', 'year': 2023.0})]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notícia 3\n",
        "\n",
        "Projeto Temático une Ciência de Dados e Sociologia no mapeamento da criminalidade"
      ],
      "metadata": {
        "id": "prErS9ZgqKFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"O que significam as siglas NEV e CeMEAI?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea634d21-9adc-49a0-b3eb-1d6ee33d8076",
        "id": "Du8Ll-Kfl3Y1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NEV significa Núcleo de Estudos da Violência da USP, e CeMEAI significa Centro de Ciências Matemáticas Aplicadas à Indústria.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Qual o nome, as frentes e quanto tempo durará o projeto desenvolvido em conjunto pelo NEV e CeMEAI?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0bb14fe1-5834-433c-a715-497965588608",
        "id": "dm2VIngYqTI_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O nome do projeto desenvolvido em conjunto pelo NEV e CeMEAI é \"Criminalidade, Insegurança e Legitimidade: uma abordagem transdisciplinar\". As frentes de pesquisa do projeto são a legitimidade e a impunidade, os padrões urbanos e criminais, além de incluir um portal de dados para organizar e analisar informações relacionadas à criminalidade, e o treinamento multidisciplinar de estudantes e pesquisadores. O projeto terá duração de cinco anos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notícia 4\n",
        "\n",
        "O avanço das pesquisas matemáticas com foco no espectro autista"
      ],
      "metadata": {
        "id": "9M_Jgme_yoLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"A pesquisa de Francisco Rodrigues utilizou a imagem cerebral de quantas pessoas?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wJqJbaGGz2i2",
        "outputId": "32df1299-bfc7-4734-98ac-52566c91f584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A pesquisa de Francisco Rodrigues utilizou a imagem cerebral de 500 pessoas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Quantas das imagens cerebrais utilizadas pela pesquisa de Francisco Rodrigues eram de pessoas no espectro autista?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aisFnFqa0JO1",
        "outputId": "62b57877-a762-44e0-ae48-1161c7f00e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A pesquisa de Francisco Rodrigues utilizou dados de imagens cerebrais de 242 pessoas pertencentes ao espectro autista.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Quantos porcento da imagens cerebrais utilizadas na pesquisa feita por Francisco Rodrigues eram referentes a pessoas no espectro autista?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mSshVB-00TLe",
        "outputId": "a79569ed-1129-496f-a8bf-dba9b32e3578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A pesquisa feita por Francisco Rodrigues utilizou dados de imagens cerebrais de 500 pessoas, sendo 242 pertencentes ao espectro autista. Portanto, 48,4% das imagens cerebrais utilizadas na pesquisa eram referentes a pessoas no espectro autista.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Obtenha a porcentagem das imagens cerebrais utilizadas na pesquisa feita por Francisco Rodrigues que eram referentes a pessoas no espectro autista e tire a raiz quadrada desse valor em porcentagem.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tl78ryIRrEM3",
        "outputId": "61487d25-95b2-475e-ba64-8d9f58247182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A porcentagem das imagens cerebrais utilizadas na pesquisa feita por Francisco Rodrigues que eram referentes a pessoas no espectro autista é de 48,4%. Portanto, a raiz quadrada desse valor em porcentagem é aproximadamente 6,96%.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perguntas que não podem ser respondidas"
      ],
      "metadata": {
        "id": "n-x_p7ecqUhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Onde Bruno nasceu?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebd7dc50-ba78-44d3-ffc4-2bc39b0941a4",
        "id": "_EYpQG8Da1Ow"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Não há informações suficientes no contexto fornecido para determinar onde Bruno nasceu.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Qual a capital do Brasil?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_4mLEnscy_ov",
        "outputId": "d5a2dcde-39a8-483d-ff89-f06b11cce2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Não há informações suficientes nos trechos de contexto fornecidos para responder à pergunta sobre qual é a capital do Brasil.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Quanto é 2 + 2?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OajCIcwxzKAa",
        "outputId": "96c5e340-cf82-40b1-bc87-9b1d8bd84602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Desculpe, mas não tenho a resposta para essa pergunta.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}