{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UsSogVxqU4vn",
        "6te7Gzgtp3Y3",
        "MClt9sKdnhn-",
        "9x0CwQUpnwEl",
        "qm6BbtU5oPrV",
        "Ajdi4oMJbCfW",
        "_C-JdQRDpBol",
        "3HWK5wrtpXhU",
        "t3dWRqENm7Fd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependências\n"
      ],
      "metadata": {
        "id": "JBALOe0fRjoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BlGnuAJYRO5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2812d9f-566a-4d5b-d925-5e89d7052f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain openai tiktoken \"pinecone-client[grpc]\" apache_beam mwparserfromhell cohere python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregar *API keys*"
      ],
      "metadata": {
        "id": "1UujmB7BRos7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from dotenv import load_dotenv\n",
        "files_uploaded = files.upload()\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "aicpbQ76RrU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a2289444-c385-4068-8312-fb697103c9ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11983e46-00a1-4217-9780-e047b0faf9e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-11983e46-00a1-4217-9780-e047b0faf9e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env to .env\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "UsSogVxqU4vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerais\n",
        "import os\n",
        "import datetime\n",
        "from uuid import uuid4\n",
        "import tiktoken\n",
        "import pinecone\n",
        "\n",
        "# LangChain\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema import BaseOutputParser"
      ],
      "metadata": {
        "id": "hIgyRluGU8sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18d7f4c-93c3-4afa-bbe2-d9490871c000"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição de funções"
      ],
      "metadata": {
        "id": "6te7Gzgtp3Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de comprimento (len) baseado em um tokenizer\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "e1DJWs2CVwtR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de inserção de dados em um index do Pinecone\n",
        "def insert_index(index, texts, metadatas, embeddings_model):\n",
        "    # Obtem ids unicos\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    # Realiza embedding dos textos\n",
        "    embeds = embeddings_model.embed_documents(texts)\n",
        "    # Insere no índice\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))"
      ],
      "metadata": {
        "id": "tzbBTYBtWJJa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que itera sobre os chunks para inseri-los no Pinecone\n",
        "def insert_pinecone(index, chunks, embedding_model):\n",
        "  batch_limit = 100\n",
        "  texts = []\n",
        "  metadatas = []\n",
        "\n",
        "  # Chunks obtidos na módulo de divisao\n",
        "  for chunk in chunks:\n",
        "      # Obter texto e metadados do chunk\n",
        "      chunk_text = chunk.page_content\n",
        "      chunk_metadatas = chunk.metadata\n",
        "      chunk_metadatas[\"year\"] = datetime.datetime.now().year\n",
        "      chunk_metadatas[\"text\"] = chunk_text\n",
        "\n",
        "      # Adiciona a lista de textos e metadados\n",
        "      texts.append(chunk_text)\n",
        "      metadatas.append(chunk_metadatas)\n",
        "\n",
        "      # Se ultrapassou o tamanho limite do lote (batch),\n",
        "      # insere no índice\n",
        "      if len(texts) >= batch_limit:\n",
        "          insert_index(index, texts, metadatas, embedding_model)\n",
        "          # Esvazia listas\n",
        "          texts = []\n",
        "          metadatas = []\n",
        "\n",
        "  # Se ainda restam dados a serem inseridos\n",
        "  if len(texts) > 0:\n",
        "      insert_index(index, texts, metadatas, embedding_model)"
      ],
      "metadata": {
        "id": "9lRAV_xxXWnz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alimentar banco de dados\n"
      ],
      "metadata": {
        "id": "mNDkXZy_TTE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar documentos\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "    \"https://jornal.usp.br/ciencias/ciencias-isolamento-e-coesao-dos-grupos-de-direita-facilitaram-propagacao-coordenada-nas-eleicoes/\",\n",
        "    \"http://www.saocarlos.usp.br/atencao-a-saude-mental-e-inclusao-na-universidade/\",\n",
        "    \"https://cemeai.icmc.usp.br/projeto-tematico-une-ciencia-de-dados-e-sociologia-no-mapeamento-da-criminalidade/\",\n",
        "    \"https://cemeai.icmc.usp.br/o-avanco-das-pesquisas-matematicas-com-foco-no-espectro-autista/\",\n",
        "    \"https://www.icmc.usp.br/noticias/6106-como-construir-um-sistema-computacional-se-uma-falha-pode-ser-fatal\"\n",
        "])\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "# Definição do tokenizer\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "# Definição do text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Divide o documento em chunks\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Verifica se é preciso criar o índice\n",
        "index_name = \"noticias-icmc\"\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # Cria novo índice\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric=\"cosine\", # Metrica de busca\n",
        "        dimension=1536  # Dimensão do embedding. 1536 para text-embedding-ada-002\n",
        "    )\n",
        "\n",
        "# Carrega o índice\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Insere chunks no Pinecone\n",
        "insert_pinecone(index, chunks, embedding_model)"
      ],
      "metadata": {
        "id": "DEccXXxcTS1Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construção de *chains*"
      ],
      "metadata": {
        "id": "-ltyInxQBrIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain1: k = 3 e exigindo resposta concisa"
      ],
      "metadata": {
        "id": "MClt9sKdnhn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = \"noticias-icmc\"\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"Use os seguintes trechos de contexto para responder à pergunta no final.\n",
        "Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "Use no máximo três frases e mantenha a resposta o mais concisa possível.\n",
        "Contexto: {contexto}\n",
        "Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain1 = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "JWOofXkE98qY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain2: k = 5 e exigindo resposta concisa"
      ],
      "metadata": {
        "id": "9x0CwQUpnwEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = \"noticias-icmc\"\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"Use os seguintes trechos de contexto para responder à pergunta no final.\n",
        "Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "Use no máximo três frases e mantenha a resposta o mais concisa possível.\n",
        "Contexto: {contexto}\n",
        "Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain2 = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "eMGigd1RntGA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain3: k = 3 e não exigindo resposta concisa"
      ],
      "metadata": {
        "id": "qm6BbtU5oPrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = \"noticias-icmc\"\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"Use os seguintes trechos de contexto para responder à pergunta no final.\n",
        "Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "Contexto: {contexto}\n",
        "Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain3 = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "d5-Tsf5MoNht"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiências novas"
      ],
      "metadata": {
        "id": "Ajdi4oMJbCfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pergunta que não está nos documentos, mas sem especificar para o LLM que ele deve dizer que não sabe\n",
        "# i.e.: forçar alucinação / resposta errada"
      ],
      "metadata": {
        "id": "ASpz-j_BYimx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pergunta direto ao LLM, sem passar o contexto"
      ],
      "metadata": {
        "id": "pDPqQdZRYxcB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiências"
      ],
      "metadata": {
        "id": "U5Q-7b9xDLDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain1: k = 3 e exigindo resposta concisa"
      ],
      "metadata": {
        "id": "_C-JdQRDpBol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Que tipo de trabalho conjunto o NEV e o CeMEAI estão desenvolvendo considerando as frentes de pesquisa desse trabalho? Inclua na resposta o que significam as siglas NEV e CeMEAI.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3746bbdc-abbd-40c0-81a1-d89d8754e368",
        "id": "Du8Ll-Kfl3Y1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O NEV (Núcleo de Estudos da Violência) e o CeMEAI (Centro de Ciências Matemáticas Aplicadas à Indústria) estão desenvolvendo um trabalho conjunto que une ciência de dados e sociologia no mapeamento da criminalidade. Esse trabalho envolve o desenvolvimento de ferramentas específicas para o problema em questão, compartilhamento de bancos de dados e perspectivas analíticas, além de introduzir novas perspectivas de análise para problemas complexos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Que tipo de trabalho conjunto o NEV e o CeMEAI estão desenvolvendo considerando as frentes de pesquisa desse trabalho?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uv3ip8uQkazD",
        "outputId": "9eb51f7d-a543-4ffc-db2b-729504de3654"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O NEV e o CeMEAI estão desenvolvendo um trabalho conjunto que envolve a criação de ferramentas matemáticas e computacionais para abordar questões relacionadas à criminalidade, impunidade e legitimidade das instituições de segurança pública. Esse trabalho também inclui o desenvolvimento de um portal de dados para analisar informações relacionadas à criminalidade e o treinamento multidisciplinar de estudantes e pesquisadores.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Que tipo de trabalho conjunto o NEV e o CeMEAI estão desenvolvendo?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d475a743-2781-4079-8364-7d40fc87a503",
        "id": "45ydNimXlAJg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O NEV e o CeMEAI estão desenvolvendo um projeto temático de ciência de dados e sociologia, com o objetivo de mapear a criminalidade e introduzir novas perspectivas de análise para problemas complexos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"O que fez com que os grupos de esquerda tivessem menos sucesso do que os grupos de direitas nas redes sociais durantes as eleições brasileiras de 2022?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_6qfzDXUbFo1",
        "outputId": "e12b9405-a9ce-4688-9288-188ab4c09157"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Os grupos de esquerda tinham uma estrutura hierárquica menos clara e estavam mais misturados com outras comunidades online, enquanto os grupos de direita eram mais isolados e coesos internamente. Isso permitiu uma hierarquia mais rígida nos grupos de direita, com poucas pessoas comunicando para um grande número de seguidores, facilitando a propagação coordenada de informações.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Qual a principal característica de um grupo político polarizado?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c741030-1ab0-4d4a-a600-b5366a795256",
        "id": "Ua-ebJZpZk1A"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A principal característica de um grupo político polarizado é o isolamento de outros grupos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"Onde Bruno nasceu?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d1a44f3-201f-46c1-d365-6a7f311fc62d",
        "id": "_EYpQG8Da1Ow"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Não é possível determinar onde Bruno nasceu com base nos trechos de contexto fornecidos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain1.invoke({\"pergunta\": \"O que ocorrerá nos dias 3 e 4 de outubro de 2023?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ggQCYQdOc1qT",
        "outputId": "d2727c52-c670-4816-ab5e-3e7d77b2a80b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Não há informações sobre o que ocorrerá nos dias 3 e 4 de outubro de 2023 nos trechos de contexto fornecidos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consegue responder mesmo com k = 3\n",
        "chain1.invoke({\"pergunta\": \"Quando ocorrerá o evento 'Atenção à Saúde Mental e Inclusão na Universidade'?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "28b7BqjWcUvC",
        "outputId": "cc57e191-839a-4175-8420-88fa16ac5ad6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"O evento 'Atenção à Saúde Mental e Inclusão na Universidade' ocorrerá nos dias 3 e 4 de outubro de 2023.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain2: k = 5 e exigindo resposta concisa\n"
      ],
      "metadata": {
        "id": "3HWK5wrtpXhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tive que aumentar de k = 3 para k = 5 para conseguir fazer a chain responder corretamente\n",
        "# Talvez porque o dado onde a informação está esteja sujo com caracteres desnecessários (\\n, telefone, etc...)\n",
        "chain2.invoke({\"pergunta\": \"O que ocorrerá nos dias 3 e 4 de outubro de 2023?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1P9254u5b8rJ",
        "outputId": "87cdbbe3-cae1-48f8-857e-81831f804483"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nos dias 3 e 4 de outubro de 2023 ocorrerá o evento \"Atenção à Saúde Mental e Inclusão na Universidade\" no Auditório Prof. Sérgio Mascarenhas, no Instituto de Física da USP de São Carlos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain3: k = 3 e não exigindo resposta concisa"
      ],
      "metadata": {
        "id": "t3dWRqENm7Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain3.invoke({\"pergunta\": \"Que tipo de trabalho conjunto o NEV e o CeMEAI estão desenvolvendo?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "41702ee8-9982-44f7-eaa6-874c89b2d3d0",
        "id": "iPeK0RRTmdws"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O NEV e o CeMEAI estão desenvolvendo um trabalho conjunto que envolve a união da ciência de dados e da sociologia no mapeamento da criminalidade.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain3.invoke({\"pergunta\": \"Que tipo de trabalho conjunto o NEV e o CeMEAI estão desenvolvendo considerando as frentes de pesquisa desse trabalho? Inclua na resposta o que significam as siglas NEV e CeMEAI.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9bc1b6de-0d7b-467b-9c6c-27e4be9bf005",
        "id": "c9QxI6yxlbZH"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O NEV (Núcleo de Estudos da Violência da USP) e o CeMEAI (Centro de Ciências Matemáticas Aplicadas à Indústria) estão desenvolvendo um trabalho conjunto no projeto temático de mapeamento da criminalidade. Esse trabalho inclui a criação de ferramentas específicas para o problema em questão, a realização de novas investigações teóricas e computacionais e a introdução de novas perspectivas de análise para problemas complexos. As siglas NEV e CeMEAI significam respectivamente Núcleo de Estudos da Violência da USP e Centro de Ciências Matemáticas Aplicadas à Indústria.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}