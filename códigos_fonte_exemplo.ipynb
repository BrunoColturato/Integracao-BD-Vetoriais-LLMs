{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPtxs7zGORlA"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai tiktoken \"pinecone-client[grpc]\" apache_beam mwparserfromhell cohere python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de como carregar chaves de API no Google Colab.\n",
        "from google.colab import files\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "files_uploaded = files.upload()\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "gIjfu2qiOaaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do WebBaseLoader para carregar documentos a partir de URLs.\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "    \"https://www.icmc.usp.br/noticias\",\n",
        "    \"https://www.icmc.usp.br/graduacao/engenharia-de-computacao\"]\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "BcSm03-LOw_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do divisor de texto CharacterTextSplitter do LangChain.\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "9wvP8ARvO7On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do divisor de texto RecursiveCharacterTextSplitter do LangChain.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "JXgtRoqRO_ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do modelo text-embedding-ada-002 da OpenAI por meio do LangChain.\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "embeddings = embeddings_model.embed_documents([\n",
        "    \"Podemos destacar ao menos três frentes de pesquisa feita \\\n",
        "    pelos CEPIDs : da legitimidade e da impunidade e dos padrões \\\n",
        "    urbanos e criminais.\"\n",
        "])\n",
        "\n",
        "embedded_query = embeddings_model.embed_query(\"Quais são as frentes \\\n",
        "                de pesquisa do projeto feito pelos CEPIDs?\")"
      ],
      "metadata": {
        "id": "KQyCQJQAPE7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do modelo de código aberto multilingual-e5-base por meio do LangChain.\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-base\"\n",
        ")\n",
        "\n",
        "embeddings = embeddings_model.embed_documents([\n",
        "    \"Podemos destacar ao menos três frentes de pesquisa feita \\\n",
        "    pelos CEPIDs : da legitimidade e da impunidade e dos padrões \\\n",
        "    urbanos e criminais.\"\n",
        "])\n",
        "\n",
        "embedded_query = embeddings_model.embed_query(\"Quais são as frentes \\\n",
        "                de pesquisa do projeto feito pelos CEPIDs?\")"
      ],
      "metadata": {
        "id": "bGNwoze5PNAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função utilizada para gerar embeddings e inserir os vetores em um índice do Pinecone.\n",
        "from uuid import uuid4\n",
        "\n",
        "def insert_index(index, texts, metadatas, embeddings_model):\n",
        "    # Gera ids unicos\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    # Realiza embedding dos textos\n",
        "    embeds = embeddings_model.embed_documents(texts)\n",
        "    # Insere no índice\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))"
      ],
      "metadata": {
        "id": "4jgNVli5PRfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Procedimento para inserção de chunks no Pinecone.\n",
        "import pinecone\n",
        "import datetime\n",
        "\n",
        "# Carrega o índice\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "batch_limit = 100\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    # Obter texto e metadados do chunk\n",
        "    chunk_text = chunk.page_content\n",
        "    chunk_metadatas = chunk.metadata\n",
        "    chunk_metadatas[\"year\"] = datetime.datetime.now().year\n",
        "    chunk_metadatas[\"text\"] = chunk_text\n",
        "\n",
        "    # Adiciona às listas de textos e metadados\n",
        "    texts.append(chunk_text)\n",
        "    metadatas.append(chunk_metadatas)\n",
        "\n",
        "    # Se ultrapassou o tamanho limite do lote (batch)\n",
        "    if len(texts) >= batch_limit:\n",
        "        insert_index(index, texts, metadatas, embeddings_model)\n",
        "        texts = []\n",
        "        metadatas = []\n",
        "\n",
        "# Se ainda restam dados a serem inseridos\n",
        "if len(texts) > 0:\n",
        "    insert_index(index, texts, metadatas, embeddings_model)"
      ],
      "metadata": {
        "id": "otrnn938PYe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de realização de consulta no Pinecone por meio do LangChain.\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "vectorstore = Pinecone.from_existing_index(index_name,\n",
        "                                           embeddings_model)\n",
        "\n",
        "query = \"O que significa a sigla CEPIDs?\"\n",
        "results = vectorstore.similarity_search(\n",
        "    query,\n",
        "    k=3,\n",
        "    filter={\n",
        "        \"language\": {\"\\$eq\": \"pt-BR\"},\n",
        "        \"year\": {\"\\$gte\": 2022}\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "E_TaY3lVPgLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de utilização do modelo gpt-3.5-turbo por meio do LangChain.\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "result = openai.predict(\"O que são Large Language Models?\")"
      ],
      "metadata": {
        "id": "1y1uU1MOPmEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de construção de prompt por meio do LangChain.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "    Use os seguintes trechos de contexto para responder à pergunta no\n",
        "    final. Se você não sabe a resposta, apenas diga que não sabe, não\n",
        "    tente inventar uma resposta.\n",
        "    Contexto: {contexto}\n",
        "    Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "prompt = prompt_template.format(\n",
        "    contexto=\"Bruno nasceu em Itapeva\",\n",
        "    pergunta=\"O que você sabe sobre Itapeva?\")\n",
        "resposta = openai.predict(prompt)"
      ],
      "metadata": {
        "id": "6OvKgeCaPtvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain completa para QA com RAG por meio do LangChain.\n",
        "\n",
        "# Carrega modelo de embedding\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Conecta ao Pinecone\n",
        "index_name = 'noticias-icmc'\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment=os.getenv(\"PINECONE_ENV\"),\n",
        ")\n",
        "\n",
        "# Carrega o index do Pinecone (vector database)\n",
        "vectorstore = Pinecone.from_existing_index(index_name,\n",
        "                                           embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Carrega LLM\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
        "\n",
        "# Define template de prompt\n",
        "template = \"\"\"\n",
        "    Use os seguintes trechos de contexto para responder à pergunta no\n",
        "    final. Se você não sabe a resposta, apenas diga que não sabe, não\n",
        "    tente inventar uma resposta.\n",
        "    Contexto: {contexto}\n",
        "    Pergunta: {pergunta}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Define chain\n",
        "chain = {\n",
        "    \"contexto\": itemgetter(\"pergunta\") | retriever,\n",
        "    \"pergunta\": itemgetter(\"pergunta\")\n",
        "} | prompt_template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "keAWUttjP1CA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}